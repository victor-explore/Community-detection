{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Program Files/Python312/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_wiki_vote_data(file_path):\n",
    "    # Initialize an empty list to store the edges\n",
    "    edges = []\n",
    "\n",
    "    # Open and read the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Skip comments and empty lines\n",
    "            if line.startswith('#') or line.strip() == '':\n",
    "                continue\n",
    "            \n",
    "            # Split the line into source and target nodes and convert them to integers\n",
    "            source, target = map(int, line.strip().split())\n",
    "            \n",
    "            # Add the edge to the list\n",
    "            edges.append([source, target])\n",
    "    \n",
    "    # Convert the list to a numpy array and remove duplicate edges\n",
    "    nodes_connectivity_list = np.unique(np.array(edges), axis=0)\n",
    "    \n",
    "    return nodes_connectivity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the file path to use a relative path\n",
    "file_path = os.path.join(\"data\", \"Wiki-Vote.txt\")\n",
    "nodes_connectivity_list_wiki = import_wiki_vote_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 entries of nodes_connectivity_list_wiki:\n",
      "[ 3 28]\n",
      "[ 3 30]\n",
      "[ 3 39]\n",
      "[ 3 54]\n",
      "[  3 108]\n",
      "\n",
      "Shape of nodes_connectivity_list_wiki: (103689, 2)\n"
     ]
    }
   ],
   "source": [
    "# Print a few entries of the nodes_connectivity_list_wiki array\n",
    "print(\"First 5 entries of nodes_connectivity_list_wiki:\")\n",
    "for i in range(min(5, len(nodes_connectivity_list_wiki))):\n",
    "    print(nodes_connectivity_list_wiki[i])\n",
    "\n",
    "print(\"\\nShape of nodes_connectivity_list_wiki:\", nodes_connectivity_list_wiki.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No repeated edges found.\n"
     ]
    }
   ],
   "source": [
    "# Check for repeated rows (edges) in nodes_connectivity_list_wiki\n",
    "unique_edges, counts = np.unique(nodes_connectivity_list_wiki, axis=0, return_counts=True)\n",
    "repeated_edges = unique_edges[counts > 1]\n",
    "\n",
    "if len(repeated_edges) > 0:\n",
    "    print(\"Repeated edges found:\")\n",
    "    for edge in repeated_edges:\n",
    "        print(edge)\n",
    "else:\n",
    "    print(\"No repeated edges found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_edge_betweenness(G):\n",
    "\n",
    "    start_time = time.time()\n",
    "    edge_betweenness = {edge: 0 for edge in G.edges()}\n",
    "    nodes = list(G.nodes())\n",
    "    total_nodes = len(nodes)\n",
    "    \n",
    "    print(f\"Starting edge betweenness calculation for {total_nodes} nodes and {len(G.edges())} edges\")\n",
    "    \n",
    "    for i, source in enumerate(nodes):\n",
    "        if i % 100 == 0:  # Print progress every 100 nodes\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Processing node {i+1}/{total_nodes} ({(i+1)/total_nodes*100:.2f}%). Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "        \n",
    "        # Run BFS from the source node\n",
    "        distances = {node: float('inf') for node in nodes}\n",
    "        distances[source] = 0\n",
    "        queue = deque([(source, 0)])\n",
    "        paths = {node: [] for node in nodes}\n",
    "        paths[source] = [[source]]\n",
    "        \n",
    "        while queue:\n",
    "            node, dist = queue.popleft()\n",
    "            for neighbor in G.neighbors(node):\n",
    "                if distances[neighbor] > dist + 1:\n",
    "                    distances[neighbor] = dist + 1\n",
    "                    paths[neighbor] = [path + [neighbor] for path in paths[node]]\n",
    "                    queue.append((neighbor, dist + 1))\n",
    "                elif distances[neighbor] == dist + 1:\n",
    "                    paths[neighbor].extend([path + [neighbor] for path in paths[node]])\n",
    "        \n",
    "        # Calculate edge betweenness\n",
    "        for target in nodes:\n",
    "            if target != source:\n",
    "                total_paths = len(paths[target])\n",
    "                if total_paths > 0:\n",
    "                    for path in paths[target]:\n",
    "                        for i in range(len(path) - 1):\n",
    "                            edge = (path[i], path[i+1])\n",
    "                            if edge in edge_betweenness:\n",
    "                                edge_betweenness[edge] += 1 / total_paths\n",
    "                            else:\n",
    "                                edge_betweenness[(path[i+1], path[i])] += 1 / total_paths\n",
    "    \n",
    "    print(\"Finished calculating edge betweenness. Normalizing results...\")\n",
    "    \n",
    "    # Normalize by the number of node pairs\n",
    "    n = len(nodes)\n",
    "    normalization_factor = 1 / ((n * (n - 1)) / 2)\n",
    "    for edge in edge_betweenness:\n",
    "        edge_betweenness[edge] *= normalization_factor\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Edge betweenness calculation completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    return edge_betweenness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Girvan_Newman_one_level(nodes_connectivity_list):\n",
    "    # Create a graph from the edge list\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(nodes_connectivity_list)\n",
    "    \n",
    "    # Calculate edge betweenness\n",
    "    edge_betweenness = calculate_edge_betweenness(G)\n",
    "    \n",
    "    # Find the edge with the highest betweenness\n",
    "    max_betweenness_edge = max(edge_betweenness, key=edge_betweenness.get)\n",
    "    \n",
    "    # Remove the edge with the highest betweenness\n",
    "    G.remove_edge(*max_betweenness_edge)\n",
    "    \n",
    "    # Find connected components (communities)\n",
    "    communities = list(nx.connected_components(G))\n",
    "    \n",
    "    # Create a mapping from original node IDs to new contiguous node IDs\n",
    "    node_mapping = {node: idx for idx, node in enumerate(G.nodes())}\n",
    "    \n",
    "    # Create the graph partition array\n",
    "    n = len(G.nodes())\n",
    "    graph_partition = np.zeros(n, dtype=int)\n",
    "    \n",
    "    for community in communities:\n",
    "        community_id = min(community)\n",
    "        for node in community:\n",
    "            graph_partition[node_mapping[node]] = node_mapping[community_id]\n",
    "    \n",
    "    return graph_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting edge betweenness calculation for 7115 nodes and 100762 edges\n",
      "Processing node 1/7115 (0.01%). Elapsed time: 0.06 seconds\n",
      "Processing node 101/7115 (1.42%). Elapsed time: 98.37 seconds\n",
      "Processing node 201/7115 (2.83%). Elapsed time: 189.58 seconds\n",
      "Processing node 301/7115 (4.23%). Elapsed time: 276.43 seconds\n",
      "Processing node 401/7115 (5.64%). Elapsed time: 366.04 seconds\n",
      "Processing node 501/7115 (7.04%). Elapsed time: 453.63 seconds\n",
      "Processing node 601/7115 (8.45%). Elapsed time: 538.41 seconds\n",
      "Processing node 701/7115 (9.85%). Elapsed time: 619.03 seconds\n",
      "Processing node 801/7115 (11.26%). Elapsed time: 697.23 seconds\n",
      "Processing node 901/7115 (12.66%). Elapsed time: 779.50 seconds\n",
      "Processing node 1001/7115 (14.07%). Elapsed time: 867.57 seconds\n",
      "Processing node 1101/7115 (15.47%). Elapsed time: 958.11 seconds\n",
      "Processing node 1201/7115 (16.88%). Elapsed time: 1058.89 seconds\n",
      "Processing node 1301/7115 (18.29%). Elapsed time: 1154.93 seconds\n",
      "Processing node 1401/7115 (19.69%). Elapsed time: 1250.64 seconds\n",
      "Processing node 1501/7115 (21.10%). Elapsed time: 1349.99 seconds\n",
      "Processing node 1601/7115 (22.50%). Elapsed time: 1444.36 seconds\n",
      "Processing node 1701/7115 (23.91%). Elapsed time: 1545.13 seconds\n",
      "Processing node 1801/7115 (25.31%). Elapsed time: 1653.97 seconds\n",
      "Processing node 1901/7115 (26.72%). Elapsed time: 1761.10 seconds\n",
      "Processing node 2001/7115 (28.12%). Elapsed time: 1867.64 seconds\n",
      "Processing node 2101/7115 (29.53%). Elapsed time: 1962.08 seconds\n",
      "Processing node 2201/7115 (30.93%). Elapsed time: 2066.55 seconds\n",
      "Processing node 2301/7115 (32.34%). Elapsed time: 2181.65 seconds\n",
      "Processing node 2401/7115 (33.75%). Elapsed time: 2284.31 seconds\n",
      "Processing node 2501/7115 (35.15%). Elapsed time: 2395.27 seconds\n",
      "Processing node 2601/7115 (36.56%). Elapsed time: 2511.90 seconds\n",
      "Processing node 2701/7115 (37.96%). Elapsed time: 2621.76 seconds\n",
      "Processing node 2801/7115 (39.37%). Elapsed time: 2737.59 seconds\n",
      "Processing node 2901/7115 (40.77%). Elapsed time: 2851.77 seconds\n",
      "Processing node 3001/7115 (42.18%). Elapsed time: 2960.03 seconds\n",
      "Processing node 3101/7115 (43.58%). Elapsed time: 3072.78 seconds\n",
      "Processing node 3201/7115 (44.99%). Elapsed time: 3187.56 seconds\n",
      "Processing node 3301/7115 (46.39%). Elapsed time: 3294.20 seconds\n",
      "Processing node 3401/7115 (47.80%). Elapsed time: 3407.72 seconds\n",
      "Processing node 3501/7115 (49.21%). Elapsed time: 3524.17 seconds\n",
      "Processing node 3601/7115 (50.61%). Elapsed time: 3628.98 seconds\n",
      "Processing node 3701/7115 (52.02%). Elapsed time: 3737.25 seconds\n",
      "Processing node 3801/7115 (53.42%). Elapsed time: 3845.91 seconds\n",
      "Processing node 3901/7115 (54.83%). Elapsed time: 3956.98 seconds\n",
      "Processing node 4001/7115 (56.23%). Elapsed time: 4070.59 seconds\n",
      "Processing node 4101/7115 (57.64%). Elapsed time: 4177.85 seconds\n",
      "Processing node 4201/7115 (59.04%). Elapsed time: 4288.63 seconds\n",
      "Processing node 4301/7115 (60.45%). Elapsed time: 4403.05 seconds\n",
      "Processing node 4401/7115 (61.86%). Elapsed time: 4515.86 seconds\n",
      "Processing node 4501/7115 (63.26%). Elapsed time: 4625.66 seconds\n"
     ]
    }
   ],
   "source": [
    "graph_partition_wiki = Girvan_Newman_one_level(nodes_connectivity_list_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting edge betweenness calculation for 7115 nodes and 100762 edges\n",
      "Processing node 1/7115 (0.01%). Elapsed time: 0.06 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Usage:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m graph_partition_wiki \u001b[38;5;241m=\u001b[39m \u001b[43mGirvan_Newman_one_level\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes_connectivity_list_wiki\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph partition after one level of Girvan-Newman algorithm:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(graph_partition_wiki)\n",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m, in \u001b[0;36mGirvan_Newman_one_level\u001b[1;34m(nodes_connectivity_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m G\u001b[38;5;241m.\u001b[39madd_edges_from(nodes_connectivity_list)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate edge betweenness\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m edge_betweenness \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_edge_betweenness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Find the edge with the highest betweenness\u001b[39;00m\n\u001b[0;32m     10\u001b[0m max_betweenness_edge \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(edge_betweenness, key\u001b[38;5;241m=\u001b[39medge_betweenness\u001b[38;5;241m.\u001b[39mget)\n",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m, in \u001b[0;36mcalculate_edge_betweenness\u001b[1;34m(G)\u001b[0m\n\u001b[0;32m     28\u001b[0m             queue\u001b[38;5;241m.\u001b[39mappend((neighbor, dist \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m distances[neighbor] \u001b[38;5;241m==\u001b[39m dist \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 30\u001b[0m             paths[neighbor]\u001b[38;5;241m.\u001b[39mextend([path \u001b[38;5;241m+\u001b[39m [neighbor] \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths[node]])\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Calculate edge betweenness\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     28\u001b[0m             queue\u001b[38;5;241m.\u001b[39mappend((neighbor, dist \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m distances[neighbor] \u001b[38;5;241m==\u001b[39m dist \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 30\u001b[0m             paths[neighbor]\u001b[38;5;241m.\u001b[39mextend([\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mneighbor\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths[node]])\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Calculate edge betweenness\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m nodes:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Graph partition after one level of Girvan-Newman algorithm:\")\n",
    "print(graph_partition_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Girvan_Newman(nodes_connectivity_list):\n",
    "    # Create a graph from the edge list\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(nodes_connectivity_list)\n",
    "    \n",
    "    n = G.number_of_nodes()\n",
    "    community_mat = []\n",
    "    \n",
    "    while G.number_of_edges() > 0:\n",
    "        # Apply one level of Girvan-Newman algorithm\n",
    "        graph_partition = Girvan_Newman_one_level(G.edges())\n",
    "        \n",
    "        # Create a mapping from original node IDs to contiguous indices\n",
    "        node_to_index = {node: i for i, node in enumerate(G.nodes())}\n",
    "        \n",
    "        # Ensure the partition is a complete list of node assignments\n",
    "        complete_partition = np.zeros(n, dtype=int)\n",
    "        for i, node in enumerate(G.nodes()):\n",
    "            if node_to_index[node] < len(graph_partition):\n",
    "                complete_partition[i] = graph_partition[node_to_index[node]]\n",
    "            else:\n",
    "                # Assign a new community ID for nodes not in graph_partition\n",
    "                complete_partition[i] = len(graph_partition)\n",
    "        \n",
    "        # Add the current partition to the community matrix\n",
    "        community_mat.append(complete_partition)\n",
    "        \n",
    "        # Find the edge with the highest betweenness\n",
    "        edge_betweenness = calculate_edge_betweenness(G)\n",
    "        max_betweenness_edge = max(edge_betweenness, key=edge_betweenness.get)\n",
    "        \n",
    "        # Remove the edge with the highest betweenness\n",
    "        G.remove_edge(*max_betweenness_edge)\n",
    "    \n",
    "    # Convert the list of partitions to a numpy array\n",
    "    community_mat = np.array(community_mat).T\n",
    "    \n",
    "    return community_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Girvan-Newman algorithm on the wiki-vote dataset\n",
    "community_mat_wiki = Girvan_Newman(nodes_connectivity_list_wiki)\n",
    "\n",
    "print(\"Community matrix after running full Girvan-Newman algorithm:\")\n",
    "print(community_mat_wiki)\n",
    "\n",
    "# Print the shape of the community matrix to understand its dimensions\n",
    "print(\"\\nShape of the community matrix:\")\n",
    "print(community_mat_wiki.shape)\n",
    "\n",
    "# Print the number of unique communities at each level\n",
    "print(\"\\nNumber of unique communities at each level:\")\n",
    "for level in range(community_mat_wiki.shape[1]):\n",
    "    unique_communities = len(np.unique(community_mat_wiki[:, level]))\n",
    "    print(f\"Level {level}: {unique_communities} communities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "def visualise_dendogram(community_mat):\n",
    "    # Transpose the community matrix to have nodes as columns and levels as rows\n",
    "    community_mat_T = community_mat.T\n",
    "\n",
    "    # Create a linkage matrix\n",
    "    linkage_matrix = linkage(community_mat_T, method='ward')\n",
    "\n",
    "    # Create a new figure with a larger size\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # Plot the dendrogram\n",
    "    dendrogram(\n",
    "        linkage_matrix,\n",
    "        leaf_rotation=90.,  # rotates the x axis labels\n",
    "        leaf_font_size=8.,  # font size for the x axis labels\n",
    "    )\n",
    "\n",
    "    plt.title('Community Structure Dendrogram')\n",
    "    plt.xlabel('Node')\n",
    "    plt.ylabel('Distance')\n",
    "\n",
    "    # Adjust layout to prevent cutting off labels\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the dendrogram as a PNG file\n",
    "    plt.savefig('community_dendrogram.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    print(\"Dendrogram saved as 'community_dendrogram.png'\")\n",
    "\n",
    "    # Close the plot to free up memory\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the dendrogram for the wiki-vote dataset\n",
    "visualise_dendogram(community_mat_wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def louvain_one_iter(nodes_connectivity_list):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create a graph from the connectivity list\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(nodes_connectivity_list)\n",
    "    \n",
    "    # Create a mapping from node IDs to contiguous indices\n",
    "    node_to_index = {node: i for i, node in enumerate(G.nodes())}\n",
    "    index_to_node = {i: node for node, i in node_to_index.items()}\n",
    "    \n",
    "    # Initialize each node to its own community\n",
    "    communities = {node: i for i, node in enumerate(G.nodes())}\n",
    "    \n",
    "    # Calculate the initial modularity\n",
    "    m = G.number_of_edges()\n",
    "    Q = modularity(G, communities)\n",
    "    \n",
    "    total_nodes = G.number_of_nodes()\n",
    "    print(f\"Starting Louvain iteration for {total_nodes} nodes\")\n",
    "    \n",
    "    # Iterate through all nodes\n",
    "    for i, node in enumerate(G.nodes()):\n",
    "        if i % 100 == 0:  # Print progress every 100 nodes\n",
    "            elapsed_time = time.time() - start_time\n",
    "            progress = (i + 1) / total_nodes\n",
    "            estimated_total_time = elapsed_time / progress\n",
    "            remaining_time = estimated_total_time - elapsed_time\n",
    "            print(f\"Processed {i+1}/{total_nodes} nodes ({progress:.2%}). \"\n",
    "                  f\"Elapsed time: {elapsed_time:.2f}s. \"\n",
    "                  f\"Estimated remaining time: {remaining_time:.2f}s\")\n",
    "        \n",
    "        best_community = communities[node]\n",
    "        best_gain = 0\n",
    "        \n",
    "        # Check the modularity gain for moving to each neighbor's community\n",
    "        for neighbor in G.neighbors(node):\n",
    "            gain = modularity_gain(G, node, communities[neighbor], communities, m)\n",
    "            if gain > best_gain:\n",
    "                best_gain = gain\n",
    "                best_community = communities[neighbor]\n",
    "        \n",
    "        # Move the node to the best community if there's an improvement\n",
    "        if best_community != communities[node]:\n",
    "            communities[node] = best_community\n",
    "    \n",
    "    # Create the final partition array\n",
    "    n = G.number_of_nodes()\n",
    "    graph_partition = np.zeros(n, dtype=int)\n",
    "    for node, community in communities.items():\n",
    "        graph_partition[node_to_index[node]] = community\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Louvain iteration completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    return graph_partition\n",
    "\n",
    "def modularity(G, communities):\n",
    "    m = G.number_of_edges()\n",
    "    Q = 0\n",
    "    for node in G.nodes():\n",
    "        for neighbor in G.neighbors(node):\n",
    "            if communities[node] == communities[neighbor]:\n",
    "                Q += 1 - G.degree(node) * G.degree(neighbor) / (2 * m)\n",
    "    return Q / (2 * m)\n",
    "\n",
    "def modularity_gain(G, node, new_community, communities, m):\n",
    "    old_community = communities[node]\n",
    "    k_i = G.degree(node)\n",
    "    k_i_in = sum(1 for neighbor in G.neighbors(node) if communities[neighbor] == new_community)\n",
    "    k_i_out = sum(1 for neighbor in G.neighbors(node) if communities[neighbor] == old_community)\n",
    "    sigma_tot = sum(G.degree(n) for n in G.nodes() if communities[n] == new_community)\n",
    "    sigma_in = sum(G.degree(n) for n in G.nodes() if communities[n] == old_community)\n",
    "    \n",
    "    gain = (k_i_in - k_i_out) / (2 * m) - k_i * (sigma_tot - sigma_in + k_i) / (2 * m * m)\n",
    "    return gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Louvain iteration for 7115 nodes\n",
      "Processed 1/7115 nodes (0.01%). Elapsed time: 0.36s. Estimated remaining time: 2591.50s\n",
      "Processed 101/7115 nodes (1.42%). Elapsed time: 8.57s. Estimated remaining time: 595.06s\n",
      "Processed 201/7115 nodes (2.83%). Elapsed time: 15.57s. Estimated remaining time: 535.58s\n",
      "Processed 301/7115 nodes (4.23%). Elapsed time: 23.58s. Estimated remaining time: 533.88s\n",
      "Processed 401/7115 nodes (5.64%). Elapsed time: 32.08s. Estimated remaining time: 537.16s\n",
      "Processed 501/7115 nodes (7.04%). Elapsed time: 41.59s. Estimated remaining time: 549.09s\n",
      "Processed 601/7115 nodes (8.45%). Elapsed time: 50.99s. Estimated remaining time: 552.65s\n",
      "Processed 701/7115 nodes (9.85%). Elapsed time: 62.01s. Estimated remaining time: 567.36s\n",
      "Processed 801/7115 nodes (11.26%). Elapsed time: 77.38s. Estimated remaining time: 609.98s\n",
      "Processed 901/7115 nodes (12.66%). Elapsed time: 87.32s. Estimated remaining time: 602.25s\n",
      "Processed 1001/7115 nodes (14.07%). Elapsed time: 99.67s. Estimated remaining time: 608.80s\n",
      "Processed 1101/7115 nodes (15.47%). Elapsed time: 112.60s. Estimated remaining time: 615.08s\n",
      "Processed 1201/7115 nodes (16.88%). Elapsed time: 122.97s. Estimated remaining time: 605.52s\n",
      "Processed 1301/7115 nodes (18.29%). Elapsed time: 132.69s. Estimated remaining time: 592.96s\n",
      "Processed 1401/7115 nodes (19.69%). Elapsed time: 142.74s. Estimated remaining time: 582.16s\n",
      "Processed 1501/7115 nodes (21.10%). Elapsed time: 150.65s. Estimated remaining time: 563.46s\n",
      "Processed 1601/7115 nodes (22.50%). Elapsed time: 159.10s. Estimated remaining time: 547.96s\n",
      "Processed 1701/7115 nodes (23.91%). Elapsed time: 164.96s. Estimated remaining time: 525.04s\n",
      "Processed 1801/7115 nodes (25.31%). Elapsed time: 169.73s. Estimated remaining time: 500.79s\n",
      "Processed 1901/7115 nodes (26.72%). Elapsed time: 175.60s. Estimated remaining time: 481.63s\n",
      "Processed 2001/7115 nodes (28.12%). Elapsed time: 180.22s. Estimated remaining time: 460.60s\n",
      "Processed 2101/7115 nodes (29.53%). Elapsed time: 185.52s. Estimated remaining time: 442.75s\n",
      "Processed 2201/7115 nodes (30.93%). Elapsed time: 187.41s. Estimated remaining time: 418.41s\n",
      "Processed 2301/7115 nodes (32.34%). Elapsed time: 190.21s. Estimated remaining time: 397.94s\n",
      "Processed 2401/7115 nodes (33.75%). Elapsed time: 195.88s. Estimated remaining time: 384.58s\n",
      "Processed 2501/7115 nodes (35.15%). Elapsed time: 198.24s. Estimated remaining time: 365.72s\n",
      "Processed 2601/7115 nodes (36.56%). Elapsed time: 200.79s. Estimated remaining time: 348.46s\n",
      "Processed 2701/7115 nodes (37.96%). Elapsed time: 203.81s. Estimated remaining time: 333.07s\n",
      "Processed 2801/7115 nodes (39.37%). Elapsed time: 205.27s. Estimated remaining time: 316.16s\n",
      "Processed 2901/7115 nodes (40.77%). Elapsed time: 206.89s. Estimated remaining time: 300.52s\n",
      "Processed 3001/7115 nodes (42.18%). Elapsed time: 208.52s. Estimated remaining time: 285.86s\n",
      "Processed 3101/7115 nodes (43.58%). Elapsed time: 209.99s. Estimated remaining time: 271.82s\n",
      "Processed 3201/7115 nodes (44.99%). Elapsed time: 211.32s. Estimated remaining time: 258.39s\n",
      "Processed 3301/7115 nodes (46.39%). Elapsed time: 212.65s. Estimated remaining time: 245.70s\n",
      "Processed 3401/7115 nodes (47.80%). Elapsed time: 213.86s. Estimated remaining time: 233.55s\n",
      "Processed 3501/7115 nodes (49.21%). Elapsed time: 215.05s. Estimated remaining time: 221.99s\n",
      "Processed 3601/7115 nodes (50.61%). Elapsed time: 215.98s. Estimated remaining time: 210.76s\n",
      "Processed 3701/7115 nodes (52.02%). Elapsed time: 217.06s. Estimated remaining time: 200.23s\n",
      "Processed 3801/7115 nodes (53.42%). Elapsed time: 219.34s. Estimated remaining time: 191.24s\n",
      "Processed 3901/7115 nodes (54.83%). Elapsed time: 221.13s. Estimated remaining time: 182.18s\n",
      "Processed 4001/7115 nodes (56.23%). Elapsed time: 223.18s. Estimated remaining time: 173.70s\n",
      "Processed 4101/7115 nodes (57.64%). Elapsed time: 224.06s. Estimated remaining time: 164.67s\n",
      "Processed 4201/7115 nodes (59.04%). Elapsed time: 224.91s. Estimated remaining time: 156.00s\n",
      "Processed 4301/7115 nodes (60.45%). Elapsed time: 226.91s. Estimated remaining time: 148.46s\n",
      "Processed 4401/7115 nodes (61.86%). Elapsed time: 229.05s. Estimated remaining time: 141.25s\n",
      "Processed 4501/7115 nodes (63.26%). Elapsed time: 230.07s. Estimated remaining time: 133.61s\n",
      "Processed 4601/7115 nodes (64.67%). Elapsed time: 230.97s. Estimated remaining time: 126.20s\n",
      "Processed 4701/7115 nodes (66.07%). Elapsed time: 233.00s. Estimated remaining time: 119.65s\n",
      "Processed 4801/7115 nodes (67.48%). Elapsed time: 233.80s. Estimated remaining time: 112.69s\n",
      "Processed 4901/7115 nodes (68.88%). Elapsed time: 234.94s. Estimated remaining time: 106.13s\n",
      "Processed 5001/7115 nodes (70.29%). Elapsed time: 235.57s. Estimated remaining time: 99.58s\n",
      "Processed 5101/7115 nodes (71.69%). Elapsed time: 236.52s. Estimated remaining time: 93.39s\n",
      "Processed 5201/7115 nodes (73.10%). Elapsed time: 237.25s. Estimated remaining time: 87.31s\n",
      "Processed 5301/7115 nodes (74.50%). Elapsed time: 238.85s. Estimated remaining time: 81.74s\n",
      "Processed 5401/7115 nodes (75.91%). Elapsed time: 240.67s. Estimated remaining time: 76.38s\n",
      "Processed 5501/7115 nodes (77.32%). Elapsed time: 241.27s. Estimated remaining time: 70.79s\n",
      "Processed 5601/7115 nodes (78.72%). Elapsed time: 242.53s. Estimated remaining time: 65.56s\n",
      "Processed 5701/7115 nodes (80.13%). Elapsed time: 243.85s. Estimated remaining time: 60.48s\n",
      "Processed 5801/7115 nodes (81.53%). Elapsed time: 245.47s. Estimated remaining time: 55.60s\n",
      "Processed 5901/7115 nodes (82.94%). Elapsed time: 246.11s. Estimated remaining time: 50.63s\n",
      "Processed 6001/7115 nodes (84.34%). Elapsed time: 246.50s. Estimated remaining time: 45.76s\n",
      "Processed 6101/7115 nodes (85.75%). Elapsed time: 247.57s. Estimated remaining time: 41.15s\n",
      "Processed 6201/7115 nodes (87.15%). Elapsed time: 248.90s. Estimated remaining time: 36.69s\n",
      "Processed 6301/7115 nodes (88.56%). Elapsed time: 249.27s. Estimated remaining time: 32.20s\n",
      "Processed 6401/7115 nodes (89.96%). Elapsed time: 250.12s. Estimated remaining time: 27.90s\n",
      "Processed 6501/7115 nodes (91.37%). Elapsed time: 251.56s. Estimated remaining time: 23.76s\n",
      "Processed 6601/7115 nodes (92.78%). Elapsed time: 252.39s. Estimated remaining time: 19.65s\n",
      "Processed 6701/7115 nodes (94.18%). Elapsed time: 252.84s. Estimated remaining time: 15.62s\n",
      "Processed 6801/7115 nodes (95.59%). Elapsed time: 254.02s. Estimated remaining time: 11.73s\n",
      "Processed 6901/7115 nodes (96.99%). Elapsed time: 254.86s. Estimated remaining time: 7.90s\n",
      "Processed 7001/7115 nodes (98.40%). Elapsed time: 255.31s. Estimated remaining time: 4.16s\n",
      "Processed 7101/7115 nodes (99.80%). Elapsed time: 255.57s. Estimated remaining time: 0.50s\n",
      "Louvain iteration completed in 255.60 seconds\n",
      "Number of communities after one Louvain iteration: 815\n",
      "First 10 entries of the partition:\n",
      "[1822 1764  946 2219 1161 1535 2204 1822 1822 1726]\n"
     ]
    }
   ],
   "source": [
    "# Run one iteration of Louvain algorithm for the wiki-vote dataset\n",
    "graph_partition_louvain_wiki = louvain_one_iter(nodes_connectivity_list_wiki)\n",
    "\n",
    "# Print the number of communities found\n",
    "unique_communities = len(np.unique(graph_partition_louvain_wiki))\n",
    "print(f\"Number of communities after one Louvain iteration: {unique_communities}\")\n",
    "\n",
    "# Print the first few entries of the partition\n",
    "print(\"First 10 entries of the partition:\")\n",
    "print(graph_partition_louvain_wiki[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph from the nodes_connectivity_list_wiki\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(nodes_connectivity_list_wiki)\n",
    "\n",
    "# Create a mapping from node IDs to contiguous indices\n",
    "node_to_index = {node: i for i, node in enumerate(G.nodes())}\n",
    "\n",
    "# Get unique communities and assign colors\n",
    "unique_communities = np.unique(graph_partition_louvain_wiki)\n",
    "color_map = plt.colormaps['tab20']\n",
    "colors = {comm: color_map(i/len(unique_communities)) for i, comm in enumerate(unique_communities)}\n",
    "\n",
    "# Assign colors to nodes based on their community\n",
    "node_colors = [colors[graph_partition_louvain_wiki[node_to_index[node]]] for node in G.nodes()]\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "nx.draw(G, pos, node_color=node_colors, node_size=20, with_labels=False, edge_color='gray', alpha=0.6, ax=ax)\n",
    "\n",
    "# Add a title\n",
    "ax.set_title(\"Wiki-Vote Network: Communities after one Louvain iteration\", fontsize=16)\n",
    "\n",
    "# Add a colorbar legend\n",
    "sm = plt.cm.ScalarMappable(cmap=color_map, norm=plt.Normalize(vmin=0, vmax=len(unique_communities)-1))\n",
    "sm.set_array([])\n",
    "cbar = fig.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Communities', fontsize=12)\n",
    "cbar.set_ticks(range(len(unique_communities)))\n",
    "cbar.set_ticklabels(unique_communities)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_lastfm_asia_data(file_path):\n",
    "\n",
    "    # Initialize an empty list to store the edges\n",
    "    edges = []\n",
    "\n",
    "    # Open and read the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Skip the header line\n",
    "        next(file)\n",
    "        for line in file:\n",
    "            # Skip comments (lines starting with '#') and empty lines\n",
    "            if line.startswith('#') or line.strip() == '':\n",
    "                continue\n",
    "            \n",
    "            # Split the line into source and target nodes\n",
    "            # and convert them to integers\n",
    "            source, target = map(int, line.strip().split(','))\n",
    "            \n",
    "            # Add the edge to the list\n",
    "            edges.append([source, target])\n",
    "    \n",
    "    # Convert the list to a numpy array and remove duplicate edges\n",
    "    nodes_connectivity_list = np.unique(np.array(edges), axis=0)\n",
    "    \n",
    "    return nodes_connectivity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 entries of the LastFM Asia dataset:\n",
      "[[   0  747]\n",
      " [   1  126]\n",
      " [   1  580]\n",
      " [   1 1222]\n",
      " [   1 2194]\n",
      " [   1 2204]\n",
      " [   1 2639]\n",
      " [   1 4257]\n",
      " [   1 5735]\n",
      " [   1 6478]]\n",
      "\n",
      "Shape of the LastFM Asia dataset: (27806, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load the LastFM Asia dataset\n",
    "lastfm_asia_edges = import_lastfm_asia_data('Data/lastfm_asia_edges.csv')\n",
    "\n",
    "# Print the first few entries of the dataset\n",
    "print(\"First 10 entries of the LastFM Asia dataset:\")\n",
    "print(lastfm_asia_edges[:10])\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(f\"\\nShape of the LastFM Asia dataset: {lastfm_asia_edges.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Girvan-Newman algorithm for one level on the LastFM Asia dataset\n",
    "graph_partition_lastfm = Girvan_Newman_one_level(lastfm_asia_edges)\n",
    "\n",
    "# Print the resulting graph partition\n",
    "print(\"Graph partition for LastFM Asia dataset (one level of Girvan-Newman):\")\n",
    "print(graph_partition_lastfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Perform the full Girvan-Newman algorithm on the LastFM Asia dataset\n",
    "community_mat_lastfm = Girvan_Newman(lastfm_asia_edges)\n",
    "\n",
    "# Print the resulting community matrix\n",
    "print(\"Community matrix for LastFM Asia dataset (full Girvan-Newman):\")\n",
    "print(community_mat_lastfm)\n",
    "\n",
    "# Visualize the dendrogram for the communities obtained\n",
    "visualise_dendogram(community_mat_lastfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform one iteration of the Louvain algorithm on the LastFM Asia dataset\n",
    "graph_partition_louvain_lastfm = louvain_one_iter(lastfm_asia_edges)\n",
    "\n",
    "# Print the resulting graph partition from the Louvain algorithm\n",
    "print(\"Graph partition for LastFM Asia dataset (one iteration of Louvain):\")\n",
    "print(graph_partition_louvain_lastfm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataAnalyticsEnvWindows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
